{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5FCRjNghKAIXOWjs76G9m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DGuilherme/PMTese/blob/main/notebooks%5CTransformer%5CSimple%5CTransformerAllDatasetsComparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow pandas scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqxFOtm9DSqd",
        "outputId": "75e512ad-1ec6-434f-d3ce-31cbeb48180b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "juMIOW_b5Jxk"
      },
      "outputs": [],
      "source": [
        "def run_predictive_maintenance(dataset_name, scaler_type='minmax', seq_length=50):\n",
        "\n",
        "  # Load data\n",
        "  column_names = ['id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 22)]\n",
        "  train_data = pd.read_csv(f'/content/drive/MyDrive/Python/predictive-maintenance-main/datasets/cmapss/train_{dataset_name}.txt/train_{dataset_name}.txt', delim_whitespace=True, header=None)\n",
        "  test_data = pd.read_csv(f'/content/drive/MyDrive/Python/predictive-maintenance-main/datasets/cmapss/test_{dataset_name}.txt/test_{dataset_name}.txt', delim_whitespace=True, header=None)\n",
        "  rul_data = pd.read_csv(f'/content/drive/MyDrive/Python/predictive-maintenance-main/datasets/cmapss/RUL_{dataset_name}.txt/RUL_{dataset_name}.txt', delim_whitespace=True, header=None)\n",
        "\n",
        "\n",
        "  train_data.columns = column_names\n",
        "  test_data.columns = column_names\n",
        "  rul_data.columns = ['RUL']\n",
        "\n",
        "  # Generate RUL for training data\n",
        "  max_cycle = train_data.groupby('id')['cycle'].max()\n",
        "  train_data = train_data.merge(max_cycle.reset_index(), on='id', suffixes=('', '_max'))\n",
        "  train_data['RUL'] = train_data['cycle_max'] - train_data['cycle']\n",
        "  train_data.drop(columns=['cycle_max'], inplace=True)\n",
        "\n",
        "  # RUL for test data\n",
        "  max_cycle_test = test_data.groupby('id')['cycle'].max().reset_index()\n",
        "  max_cycle_test.columns = ['id', 'max_cycle']\n",
        "  rul_data.columns = ['RUL']\n",
        "  max_cycle_test['RUL'] = rul_data['RUL']\n",
        "  test_data = test_data.merge(max_cycle_test, on='id')\n",
        "  test_data['RUL'] = test_data['RUL'] + test_data['max_cycle'] - test_data['cycle']\n",
        "  test_data.drop(columns=['max_cycle'], inplace=True)\n",
        "\n",
        "  useful_sensor_cols = ['setting1', 'setting2', 'setting3'] + \\\n",
        "      [f'sensor{i}' for i in [2, 3, 4, 7, 8, 11, 12, 13, 14, 15, 17, 20, 21]]\n",
        "\n",
        "  train_data = train_data[['id', 'cycle'] + useful_sensor_cols + ['RUL']]\n",
        "  test_data = test_data[['id', 'cycle'] + useful_sensor_cols + ['RUL']]\n",
        "\n",
        "  # Choose scaler based on scaler_type\n",
        "  if scaler_type == 'standard':\n",
        "      scaler = StandardScaler()\n",
        "  elif scaler_type == 'minmax':\n",
        "      scaler = MinMaxScaler()\n",
        "  else:\n",
        "      raise ValueError(\"Invalid scaler_type. Choose 'standard' or 'minmax'.\")\n",
        "\n",
        "  # Normalize sensor values\n",
        "  scaler = StandardScaler()\n",
        "  train_data[useful_sensor_cols] = scaler.fit_transform(train_data[useful_sensor_cols])\n",
        "  test_data[useful_sensor_cols] = scaler.transform(test_data[useful_sensor_cols])\n",
        "\n",
        "\n",
        "  # Create sequences\n",
        "  def create_sequences(data, seq_length=50):\n",
        "      sequences = []\n",
        "      for unit in data['id'].unique():\n",
        "          unit_data = data[data['id'] == unit].reset_index(drop=True)\n",
        "          for start in range(len(unit_data) - seq_length + 1):\n",
        "              end = start + seq_length\n",
        "              seq_X = unit_data.iloc[start:end, 2:-1].values  # all sensor + setting cols\n",
        "              seq_y = unit_data.iloc[end-1]['RUL']\n",
        "              sequences.append((seq_X, seq_y))\n",
        "      return sequences\n",
        "\n",
        "  seq_length = 50\n",
        "  train_seqs = create_sequences(train_data, seq_length)\n",
        "  test_seqs = create_sequences(test_data, seq_length)\n",
        "\n",
        "  train_X, val_X, train_y, val_y = train_test_split(\n",
        "      [seq[0] for seq in train_seqs],\n",
        "      [seq[1] for seq in train_seqs],\n",
        "      test_size=0.2,\n",
        "      random_state=42\n",
        "  )\n",
        "\n",
        "  # Convert to numpy arrays\n",
        "  train_X = np.array(train_X)\n",
        "  train_y = np.array(train_y)\n",
        "  val_X = np.array(val_X)\n",
        "  val_y = np.array(val_y)\n",
        "  test_X = np.array([seq[0] for seq in test_seqs])\n",
        "  test_y = np.array([seq[1] for seq in test_seqs])\n",
        "\n",
        "  # Define Transformer\n",
        "  class TimeSeriesTransformer(tf.keras.Model):\n",
        "      def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):\n",
        "          super(TimeSeriesTransformer, self).__init__()\n",
        "          self.input_proj = tf.keras.layers.Dense(model_dim)\n",
        "\n",
        "          self.encoder_layers = []\n",
        "          for _ in range(num_layers):\n",
        "              self.encoder_layers.append([\n",
        "                  tf.keras.layers.LayerNormalization(),\n",
        "                  tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=model_dim),\n",
        "                  tf.keras.Sequential([\n",
        "                      tf.keras.layers.Dense(model_dim * 4, activation='relu'),\n",
        "                      tf.keras.layers.Dense(model_dim)\n",
        "                  ]),\n",
        "              ])\n",
        "\n",
        "          self.global_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
        "          self.output_layer = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "      def call(self, inputs, training=False):\n",
        "          x = self.input_proj(inputs)\n",
        "\n",
        "          for norm, mha, ffn in self.encoder_layers:\n",
        "              attn_output = mha(x, x)\n",
        "              x = norm(x + attn_output)\n",
        "              ff_output = ffn(x)\n",
        "              x = norm(x + ff_output)\n",
        "\n",
        "          x = self.global_pool(x)\n",
        "          return self.output_layer(x)\n",
        "\n",
        "  # Instantiate model\n",
        "  input_dim = train_X.shape[2]\n",
        "  model_dim = 64\n",
        "  num_heads = 8\n",
        "  num_layers = 4\n",
        "  output_dim = 1\n",
        "\n",
        "  model = TimeSeriesTransformer(input_dim, model_dim, num_heads, num_layers, output_dim)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='mse',\n",
        "                metrics=['mae'])\n",
        "\n",
        "  # Train\n",
        "  history = model.fit(\n",
        "      train_X, train_y,\n",
        "      validation_data=(val_X, val_y),\n",
        "      epochs=20,\n",
        "      batch_size=32\n",
        "  )\n",
        "\n",
        "  # Predict\n",
        "  predictions = model.predict(test_X)\n",
        "\n",
        "  # Evaluate\n",
        "  return model, predictions, mean_squared_error(test_y, predictions), mean_absolute_error(test_y, predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_datasets(datasets, scaler_type='standard', seq_length=50):\n",
        "    results = []\n",
        "    for dataset_name in datasets:\n",
        "        model, predictions, mse, mae = run_predictive_maintenance(dataset_name, scaler_type, seq_length)\n",
        "        rmse = np.sqrt(mse)  # Calculate RMSE\n",
        "        results.append([dataset_name, rmse, mae])\n",
        "\n",
        "    # Create a pandas DataFrame for the results\n",
        "    results_df = pd.DataFrame(results, columns=['Dataset', 'RMSE', 'MAE'])\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "Wbf3HKA9JRM7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of datasets\n",
        "datasets = ['FD001', 'FD002', 'FD003', 'FD004']  # Add your dataset names here\n",
        "\n",
        "# Run the comparison\n",
        "results_df = compare_datasets(datasets, scaler_type='minmax', seq_length=30)\n",
        "\n",
        "# Display the results table\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "PsezndWlJUsJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}