{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwbG2J6OhmuW/F715rff1E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DGuilherme/PMTese/blob/main/LSTMAllDatasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "hUpjH5nRZ3xp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV6w03jTWffT",
        "outputId": "b61e3900-1e79-4b42-d335-13ebb4295ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dypoSCpacJ6",
        "outputId": "db84694d-1313-4a85-b760-028bb2c6e29b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare all datasets"
      ],
      "metadata": {
        "id": "CeCD2aVSaUTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_predictive_maintenance(dataset_name, scaler_type='minmax', seq_length=50):\n",
        "  # Load data\n",
        "  column_names = ['id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 22)]\n",
        "  train_data = pd.read_csv(f'/content/drive/MyDrive/Python/predictive-maintenance-main/datasets/cmapss/train_{dataset_name}.txt/train_{dataset_name}.txt', delim_whitespace=True, header=None)\n",
        "  test_data = pd.read_csv(f'/content/drive/MyDrive/Python/predictive-maintenance-main/datasets/cmapss/test_{dataset_name}.txt/test_{dataset_name}.txt', delim_whitespace=True, header=None)\n",
        "  rul_data = pd.read_csv(f'/content/drive/MyDrive/Python/predictive-maintenance-main/datasets/cmapss/RUL_{dataset_name}.txt/RUL_{dataset_name}.txt', delim_whitespace=True, header=None)\n",
        "\n",
        "  train_data.columns = column_names\n",
        "  test_data.columns = column_names\n",
        "  rul_data.columns = ['RUL']\n",
        "\n",
        "  # Generate RUL for training data\n",
        "  #max_cycle = train_data.groupby('id')['cycle'].max().reset_index().rename(columns={'cycle': 'cycle_max'})\n",
        "  #train_data = train_data.merge(max_cycle, on='id')\n",
        "  #train_data['RUL'] = train_data['cycle_max'] - train_data['cycle']\n",
        "  #train_data.drop(columns=['cycle_max'], inplace=True)\n",
        "\n",
        "  # Generate RUL for test data\n",
        "  #max_cycle_test = test_data.groupby('id')['cycle'].max().reset_index().rename(columns={'cycle': 'cycle_max'})\n",
        "  # Merge last observed cycle and RUL on 'id'\n",
        "  #test_data = test_data.merge(max_cycle_test, on='id')\n",
        "  #test_data = test_data.merge(rul_data, on='id')\n",
        "\n",
        "  #test_data['RUL'] = test_data['RUL'] + test_data['max_cycle'] - test_data['cycle']\n",
        "  #test_data.drop(columns=['max_cycle'], inplace=True)\n",
        "\n",
        "  # --- Generate RUL for training data ---\n",
        "  max_cycle = train_data.groupby('id')['cycle'].max().reset_index().rename(columns={'cycle': 'cycle_max'})\n",
        "  train_data = train_data.merge(max_cycle, on='id')\n",
        "\n",
        "  if use_piecewise_rul:\n",
        "      train_data['RUL'] = (train_data['cycle_max'] - train_data['cycle']).clip(upper=max_rul_cap)\n",
        "  else:\n",
        "      train_data['RUL'] = train_data['cycle_max'] - train_data['cycle']\n",
        "\n",
        "  train_data.drop(columns=['cycle_max'], inplace=True)\n",
        "\n",
        "  # --- Generate RUL for test data ---\n",
        "  max_cycle_test = test_data.groupby('id')['cycle'].max().reset_index().rename(columns={'cycle': 'max_cycle'})\n",
        "  test_data = test_data.merge(max_cycle_test, on='id')\n",
        "  test_data = test_data.merge(rul_data, on='id')  # assumes rul_data has columns ['id', 'RUL']\n",
        "\n",
        "  # Adjust RUL based on current cycle\n",
        "  test_data['RUL'] = test_data['RUL'] + test_data['max_cycle'] - test_data['cycle']\n",
        "\n",
        "  # Apply piecewise cap if needed\n",
        "  if use_piecewise_rul:\n",
        "      test_data['RUL'] = test_data['RUL'].clip(upper=max_rul_cap)\n",
        "\n",
        "  test_data.drop(columns=['max_cycle'], inplace=True)\n",
        "\n",
        "  # Select useful features (as suggested in papers)\n",
        "  useful_sensor_cols = ['setting1', 'setting2', 'setting3'] + \\\n",
        "      [f'sensor{i}' for i in [2, 3, 4, 7, 8, 11, 12, 13, 14, 15, 17, 20, 21]]\n",
        "\n",
        "  train_data = train_data[['id', 'cycle'] + useful_sensor_cols + ['RUL']]\n",
        "  test_data = test_data[['id', 'cycle'] + useful_sensor_cols + ['RUL']]\n",
        "\n",
        "  # Normalize sensor values\n",
        "  scaler = MinMaxScaler()\n",
        "  train_data[useful_sensor_cols] = scaler.fit_transform(train_data[useful_sensor_cols])\n",
        "  test_data[useful_sensor_cols] = scaler.transform(test_data[useful_sensor_cols])\n",
        "\n",
        "  def create_sequences(data, seq_length=50):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    ids = data['id'].unique()\n",
        "\n",
        "    for id in ids:\n",
        "        id_data = data[data['id'] == id].reset_index(drop=True)\n",
        "        num_cycles = len(id_data)\n",
        "\n",
        "        if num_cycles < seq_length:\n",
        "            continue\n",
        "\n",
        "        for start in range(num_cycles - seq_length):\n",
        "            seq = id_data.iloc[start:start+seq_length][useful_sensor_cols].values\n",
        "            label = id_data.iloc[start+seq_length-1]['RUL']\n",
        "\n",
        "            sequences.append(seq)\n",
        "            labels.append(label)\n",
        "\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "  # Create sequences for training and testing\n",
        "  seq_length = 50\n",
        "  X_train, y_train = create_sequences(train_data, seq_length)\n",
        "  X_test, y_test = create_sequences(test_data, seq_length)\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Build the LSTM model\n",
        "  model = Sequential([\n",
        "      LSTM(100, return_sequences=True, input_shape=(seq_length, len(useful_sensor_cols))),\n",
        "      Dropout(0.2),\n",
        "      LSTM(50, return_sequences=False),\n",
        "      Dropout(0.2),\n",
        "      Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mse')\n",
        "  model.summary()\n",
        "\n",
        "  # Train the model\n",
        "  history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val))\n",
        "\n",
        "  # Predictions\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "  # Calculate metrics\n",
        "  rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "  print(f'RMSE: {rmse}')\n",
        "  print(f'MAE: {mae}')\n",
        "\n",
        "  return model, y_pred, rmse, mae\n",
        "\n"
      ],
      "metadata": {
        "id": "351EuBlVanPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_datasets(datasets, usefullfeatures, scaler_type='standard', seq_length=50,use_piecewise_rul = True):\n",
        "  results = []\n",
        "  max_rul_cap = 125\n",
        "  for dataset_name in datasets:\n",
        "    model, predictions, mse, mae = run_predictive_maintenance(dataset_name, scaler_type, seq_length)\n",
        "    rmse = np.sqrt(mse)  # Calculate RMSE\n",
        "    results.append([dataset_name, rmse, mae])\n",
        "\n",
        "  # Create a pandas DataFrame for the results\n",
        "  results_df = pd.DataFrame(results, columns=['Dataset', 'RMSE', 'MAE'])\n",
        "  use_piecewise_rul = False\n",
        "  return results_df"
      ],
      "metadata": {
        "id": "blx0ulRpapmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of datasets\n",
        "#datasets = ['FD001', 'FD002', 'FD003', 'FD004']\n",
        "datasets = ['FD001']\n",
        "\n",
        "fd001_usefullfeatures_correlation = ['sensor2', 'sensor3', 'sensor4', 'sensor7', 'sensor8', 'sensor9', 'sensor11', 'sensor12', 'sensor13', 'sensor15', 'sensor17', 'sensor20', 'sensor21']\n",
        "fd002_usefullfeatures_correlation = ['setting1', 'setting2', 'setting3', 'sensor2', 'sensor8', 'sensor14']\n",
        "fd001_usefullfeatures_boruta = ['sensor2', 'sensor3', 'sensor4', 'sensor7', 'sensor9', 'sensor11', 'sensor12', 'sensor14', 'sensor15', 'sensor20', 'sensor21']\n",
        "fd002_usefullfeatures_boruta = ['sensor2', 'sensor3', 'sensor4', 'sensor7', 'sensor8', 'sensor9', 'sensor11', 'sensor12', 'sensor13', 'sensor14', 'sensor15', 'sensor21']\n",
        "\n",
        "\n",
        "\n",
        "# Run the comparison\n",
        "results_df = compare_datasets(datasets, fd001_usefullfeatures_correlation, scaler_type='minmax', seq_length=60)\n",
        "\n",
        "# Display the results table\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "qhT2bqkHasw7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}